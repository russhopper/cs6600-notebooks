{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Decision trees\n",
    "Decisions trees are simple yet powerful learning models. They partition the input space into many partitions: some with single examples, some with more. Their implementation can get a bit complex. Let's implement a decision tree a step by step. When done, we will pull everything together in a class. Here we are using the the ID3 (Iterative Dichotomiser 3) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mylib as my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dataset\n",
    "We'll start by using the party example from the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  deadline party lazy      y\n",
       "0   Urgent   Yes  Yes  Party\n",
       "1   Urgent    No  Yes  Study\n",
       "2     Near   Yes  Yes  Party\n",
       "3     None   Yes   No  Party\n",
       "4     None    No  Yes    Pub\n",
       "5     None   Yes   No  Party\n",
       "6     Near    No   No  Study\n",
       "7     Near    No  Yes     TV\n",
       "8     Near   Yes  Yes  Party\n",
       "9   Urgent    No   No  Study"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'deadline': ['Urgent', 'Urgent', 'Near', 'None', 'None', 'None', 'Near', 'Near', 'Near', 'Urgent'],\n",
    "    'party': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No'],\n",
    "    'lazy': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No'],\n",
    "    'activity': ['Party', 'Study', 'Party', 'Party', 'Pub', 'Party', 'Study', 'TV', 'Party', 'Study']\n",
    "})\n",
    "\n",
    "ds = my.DataSet(df, y=True)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "The best way we know to build a decision tree is a greedy algorithm that builds the tree one feature (or node) at a time starting with the **best feature** at the moment. One way of specifying that is by quantifying the impurity of a node and selecting the feature that yields the least impurity. Here we have two such measures: **entropy** and **gini index**. \n",
    "\n",
    "### Using the entropy\n",
    "\n",
    "\n",
    " Apparently gain is how important a feature is... or something like that\n",
    "\n",
    "Given a set of examples $D$ with labels $y_i$ where $i \\in \\{1, 2,\\dots,L\\}$ and $L$ is the number of unique labels, the entropy is defined as:\n",
    "\n",
    "$${\\displaystyle \\mathrm {H(D)}=-\\sum _{i=1}^{L}{p_{i}\\log p_{i}}}$$\n",
    "\n",
    "We use the entropy to calculate the information gain of a feature $F$ as follows:\n",
    "\n",
    "$${\\displaystyle \\operatorname {Gain(F,D)} =H(D) -\\sum_{f \\in values(F)}\\frac{|D_f|}{|D|}H(D_f)}$$\n",
    "\n",
    "where $D_f$ is the set of all examples in $D$ where feature $F$ has the value $f$. Let's create function to calculate the information gain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(df):\n",
    "    p = df.iloc[:, -1].value_counts() / len(df)\n",
    "    return (-p * np.log2(p)).sum()\n",
    "\n",
    "def info_gain(df, feature):\n",
    "    # ratio of relative frequency for given feature (col)\n",
    "    p = df[feature].value_counts() / len(df)\n",
    "    \n",
    "    for v in p.index:\n",
    "        p.loc[v] *= entropy(df[df[feature] == v])\n",
    "        \n",
    "    return entropy(df) - p.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the gains for the three features of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deadline    0.534498\n",
       "party       1.000000\n",
       "lazy        0.209987\n",
       "dtype: float64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series({f: info_gain(df, f) for f in df.columns[:-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `party` feature has the largest gain, it becomes the root of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best feature\n",
    "This feature will be the with the maximum $Gain$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature(df):\n",
    "    features = df.iloc[:, :-1].columns\n",
    "    info = pd.DataFrame({\"feature\": features})\n",
    "\n",
    "    info['gain'] = [info_gain(df, f) for f in features]\n",
    "    return info['feature'][info['gain'].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here is the best feature for to start the tree with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'party'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_feature(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the tree\n",
    "There are many ways to constructing the tree. Here, we'll use a Pandas Series object as a dictionary-like data structure to recursively (in a nested-way) store nodes of the tree. The first index of this Series object has the root node.\n",
    "\n",
    "* First we find the best feature to add to the tree. The name of the feature will be the index and the value will be a Series object whose elements are either leaves (represented by tuples) or other series object for internal nodes.\n",
    "\n",
    "* From the best feature, we get the all the unique values of the best feature. For each unique value:\n",
    "    * Filter the data based on this value\n",
    "    * Find the number of labels in the filtered dataset. \n",
    "    * If the number of labels is 1 then append a leaf as a child to this best feature\n",
    "    * If the number of labels is > 1 then:\n",
    "        * Find the next best feature using the filtered data\n",
    "        * Repeat the process using the new best feature\n",
    "\n",
    "Here is implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = pd.Series(dtype=object)\n",
    "def make_tree(df, node, feature=None):\n",
    "    if feature is None:\n",
    "        feature = best_feature(df)\n",
    "\n",
    "    node[feature] = pd.Series(dtype=object)\n",
    "\n",
    "    fvalues = df[feature].unique()\n",
    "    for v in fvalues:\n",
    "        d = df[df[feature] == v]\n",
    "        n_classes = len(d.iloc[:, -1].unique())\n",
    "        if n_classes == 1:\n",
    "            node[feature][v] = ('L', d.iloc[:, -1].iloc[0])\n",
    "        elif n_classes > 1:\n",
    "            d = d.drop([feature], axis=1)\n",
    "            if len(d.columns) == 1:\n",
    "                node[feature][v] = ('L', d.iloc[:, -1].mode()[0])\n",
    "            else:\n",
    "                next_best_feature = best_feature(d)\n",
    "                node[feature][v] = pd.Series(dtype=object)\n",
    "                make_tree(d, node[feature][v], next_best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function to print the tree in an **inorder** manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(name, tree, d=1):        \n",
    "    for f in tree.index:\n",
    "        if isinstance(tree[f], tuple):\n",
    "            print('   ' * d, f, ' => ', tree[f], sep='')\n",
    "        else:\n",
    "            print('   ' * d, f, ': ', sep='')\n",
    "            print_tree(f, tree[f], d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the tree built using the `make_tree` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "   party: \n",
      "      Yes => ('L', 'Party')\n",
      "      No: \n",
      "         deadline: \n",
      "            Urgent => ('L', 'Study')\n",
      "            None => ('L', 'Pub')\n",
      "            Near: \n",
      "               lazy: \n",
      "                  No => ('L', 'Study')\n",
      "                  Yes => ('L', 'TV')\n"
     ]
    }
   ],
   "source": [
    "make_tree(df, tree)\n",
    "print('\\n\\n')\n",
    "print_tree(\"\", tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tree representation corresponds to the following tree:\n",
    "\n",
    "```\n",
    "                              Feature:Party\n",
    "                             /             \\\n",
    "                        Yes /               \\ No\n",
    "                           /                 \\   \n",
    "                      Leaf:Party       Feature:Deadline\n",
    "                                      /       |        \\\n",
    "                              Urgent /        | None    \\ Near\n",
    "                                    /         |          \\\n",
    "                               Leaf:Study  Leaf:Pub       \\\n",
    "                                                           \\\n",
    "                                                      Feature:Lazy\n",
    "                                                     /            \\\n",
    "                                                 No /              \\ Yes\n",
    "                                                   /                \\\n",
    "                                              Leaf:Study          Leaf:TV\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tree\n",
    "With a tree like this, how do we use it to predict the label of an unseen example? Well, it's a tree; so we use the unseen example to traverse the tree from the root until we reach a leaf. When we do reach a leaf, we report the majority label of all the examples under that leaf.\n",
    "\n",
    "Here is a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict(unseen, node=None):\n",
    "    \"\"\"\n",
    "    Returns the most probable label (or class) for each unseen input. \n",
    "    It can also be a single Series for a single example or a data frame \n",
    "    with many examples.\n",
    "    \"\"\"\n",
    "    if unseen.ndim == 1:\n",
    "        if node is None:\n",
    "            node = tree\n",
    "            \n",
    "        feature = node.index[0]\n",
    "        children = node[feature]\n",
    "        value = unseen[feature]\n",
    "        \n",
    "        for c in children.index:\n",
    "            if c == value:\n",
    "                if isinstance(children[c], tuple):\n",
    "                    return children[c][1]\n",
    "                else:\n",
    "                    return predict(unseen, children[c])\n",
    "    else:\n",
    "        return np.array([predict(unseen.iloc[i,:]) for i in range(len(unseen))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TV'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(pd.Series(['Near', 'No', 'Yes'], index=df.columns[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training confusion matrix and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 1]]\n",
      "Training accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "cm = my.confusion_matrix(df.iloc[:,-1].values, predict(df.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "print(cm)\n",
    "print('Training accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 100% accuracy. Is that a good thing, though?\n",
    "\n",
    "## Putting things together\n",
    "Let's put everything we did so far togeth into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\" A basic ID3 Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.tree = pd.Series(dtype=object)\n",
    "        self.make_tree(self.dataset.examples, self.tree)\n",
    "    \n",
    "    def entropy(self, df):\n",
    "        p = df.iloc[:, -1].value_counts() / len(df)\n",
    "        return (-p * np.log2(p)).sum()\n",
    "\n",
    "    def info_gain(self, df, feature):\n",
    "        p = df[feature].value_counts() / len(df)\n",
    "\n",
    "        for v in p.index:\n",
    "            p.loc[v] *= self.entropy(df[df[feature] == v])\n",
    "\n",
    "        return self.entropy(df) - p.sum()\n",
    "\n",
    "    def best_feature(self, df):\n",
    "        features = df.iloc[:, :-1].columns\n",
    "        info = pd.DataFrame({\"feature\": features})\n",
    "        info['gain'] = [self.info_gain(df, f) for f in features]\n",
    "        return info['feature'][info['gain'].argmax()]\n",
    "    \n",
    "\n",
    "    def print_tree(self, name='', node=None, depth=1):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "            \n",
    "        for f in node.index:\n",
    "            if isinstance(node[f], tuple):\n",
    "                print(' ' * depth, f, ' => ', node[f], sep='')\n",
    "            else:\n",
    "                print(' ' * depth, f, ': ', sep='')\n",
    "                print_tree(f, node[f], depth + 1)\n",
    "\n",
    "    def make_tree(self, df, node, feature=None):\n",
    "        if feature is None:\n",
    "            feature = self.best_feature(df)\n",
    "\n",
    "        node[feature] = pd.Series(dtype=object)\n",
    "\n",
    "        fvalues = df[feature].unique()\n",
    "        for v in fvalues:\n",
    "            d = df[df[feature] == v]\n",
    "            n_classes = len(d.iloc[:, -1].unique())\n",
    "            if n_classes == 1:\n",
    "                node[feature][v] = ('L', d.iloc[:, -1].iloc[0])\n",
    "            elif n_classes > 1:\n",
    "                d = d.drop([feature], axis=1)\n",
    "                if len(d.columns) == 1: \n",
    "                    node[feature][v] = ('L', d.iloc[:, -1].mode()[0])\n",
    "                else:\n",
    "                    next_best_feature = self.best_feature(d)\n",
    "                    node[feature][v] = pd.Series(dtype=object)\n",
    "                    self.make_tree(d, node[feature][v] ,next_best_feature)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def predict(self, unseen, node=None):\n",
    "        \"\"\"\n",
    "        Returns the most probable label (or class) for each unseen input. \n",
    "        It can also be a single Series for a single example or a data frame \n",
    "        with many examples.\n",
    "        \"\"\"\n",
    "        if unseen.ndim == 1:\n",
    "            if node is None:\n",
    "                node = self.tree\n",
    "\n",
    "            feature = node.index[0]\n",
    "            children = node[feature]\n",
    "            value = unseen[feature]\n",
    "            for c in children.index:\n",
    "                if c == value:\n",
    "                    if isinstance(children[c], tuple):\n",
    "                        return children[c][1]\n",
    "                    else:\n",
    "                        return self.predict(unseen, children[c])\n",
    "        else:\n",
    "            return np.array([self.predict(unseen.iloc[i,:]) for i in range(len(unseen))]) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.tree)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if by taking the party dataset and dividing it into a three-example test set and a seven-example training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: \n",
      "   deadline party lazy      y\n",
      "9   Urgent    No   No  Study\n",
      "7     Near    No  Yes     TV\n",
      "0   Urgent   Yes  Yes  Party\n",
      "1   Urgent    No  Yes  Study\n",
      "4     None    No  Yes    Pub\n",
      "8     Near   Yes  Yes  Party\n",
      "2     Near   Yes  Yes  Party\n",
      "\n",
      "Test set: \n",
      "   deadline party lazy      y\n",
      "6     Near    No   No  Study\n",
      "5     None   Yes   No  Party\n",
      "3     None   Yes   No  Party\n"
     ]
    }
   ],
   "source": [
    "train, test = ds.train_test_split(start=0, end=3, shuffle=True)\n",
    "print(\"Training set: \\n\", train)\n",
    "print(\"\\nTest set: \\n\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV\n",
      "[[3 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 1]]\n",
      "Training accuracy:  1.0\n",
      "[[0 2 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]]\n",
      "Test accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(train)\n",
    "print(dt.predict(pd.Series(['Near', 'No', 'Yes'], index=df.columns[:-1])))\n",
    "\n",
    "cm = my.confusion_matrix(train.examples.iloc[:,-1].values, dt.predict(train.examples.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(cm)\n",
    "print('Training accuracy: ', accuracy)\n",
    "\n",
    "cm = my.confusion_matrix(test.examples.iloc[:,-1].values, dt.predict(test.examples.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(cm)\n",
    "print('Test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you might get an error when you test, simply because the tree might not return a label. I have left it this way so you can see this issue.\n",
    "\n",
    "## CHALLENGE\n",
    "**PART 1**: Test the class above using the `datasets/restaurant.csv` dataset. Report both the training and test accuracy.\n",
    "\n",
    "**PART 2**: If instead of the information gain:\n",
    "$${\\displaystyle \\operatorname {Gain(F,D)} =H(D) -\\sum_{f \\in values(F)}\\frac{|D_f|}{|D|}H(D_f)}$$\n",
    "\n",
    "we use the weighted entropy:\n",
    "$${\\displaystyle \\operatorname {Entropy(F,D)} = \\sum_{f \\in values(F)}\\frac{|D_f|}{|D|}H(D_f)}$$\n",
    "\n",
    "to determine the best feature. Does that work? Try answering that by \n",
    "* removing the `info_gain` method of the above class and replacing it with `weighted_entropy` as described above. Keep in mind that in this case, the best feature is the one with the minimum weighted entropy.\n",
    "* testing the modified class using the same restaurant training and test sets from PART 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Testing, training, and accuracy\n",
    "Read the restaurant.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has 12 lines\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/restaurant.csv')\n",
    "print(f'Dataframe has {len(df)} lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alternate</th>\n",
       "      <th>Bar</th>\n",
       "      <th>Fri/Sat</th>\n",
       "      <th>Hungry</th>\n",
       "      <th>Patrons</th>\n",
       "      <th>Price</th>\n",
       "      <th>Raining</th>\n",
       "      <th>Reservation</th>\n",
       "      <th>Type</th>\n",
       "      <th>WaitEstimate</th>\n",
       "      <th>WillWait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>30-60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Some</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Burger</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Alternate  Bar Fri/Sat Hungry Patrons Price Raining Reservation    Type  \\\n",
       "0       Yes   No      No    Yes    Some   $$$      No         Yes  French   \n",
       "1       Yes   No      No    Yes    Full     $      No          No    Thai   \n",
       "2        No  Yes      No     No    Some     $      No          No  Burger   \n",
       "\n",
       "  WaitEstimate WillWait  \n",
       "0         0-10      Yes  \n",
       "1        30-60       No  \n",
       "2         0-10      Yes  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataSet object with the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = my.DataSet(df, y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length:     7\n",
      "Testing length:     3\n"
     ]
    }
   ],
   "source": [
    "train_1, test_1 = ds.train_test_split(test_portion=.3, shuffle=True, random_state=42)\n",
    "print(f'Training length: {len(train.examples):>5}\\nTesting length: {len(test.examples):>5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_1 = DecisionTreeClassifier(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 0]\n",
      " [0 4]]\n",
      "Training accuracy:  1.0\n",
      "[[1 0]\n",
      " [1 1]]\n",
      "Test accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "cm = my.confusion_matrix(train_1.examples.iloc[:,-1].values, dt_1.predict(train_1.examples.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(cm)\n",
    "print('Training accuracy: ', accuracy)\n",
    "\n",
    "cm = my.confusion_matrix(test_1.examples.iloc[:,-1].values, dt_1.predict(test_1.examples.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(cm)\n",
    "print('Test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got 66% accuracy using information gain as our determinent of root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using weighted entropy instead of information gain\n",
    "Make a modified `DecisionTreeClassifier_WE` (weighted entropy) class that uses weighted entropy instead of information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier_WE(DecisionTreeClassifier):\n",
    "    \"\"\" A modification of the DecisionTreeClassifier. A child really, with a large inheritence.\"\"\"\n",
    "    \n",
    "    def weighted_entropy(self, df, feature):\n",
    "        p = df[feature].value_counts() / len(df)\n",
    "        for v in p.index:\n",
    "            p.loc[v] *= self.entropy(df[df[feature] == v])\n",
    "        return p.sum()\n",
    "\n",
    "    def best_feature(self, df):\n",
    "        features = df.iloc[:, :-1].columns\n",
    "        info = pd.DataFrame({\"feature\": features})\n",
    "        info['weighted_entropy'] = [self.weighted_entropy(df, f) for f in features]\n",
    "        return info['feature'][info['weighted_entropy'].argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2 = DecisionTreeClassifier_WE(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 0]\n",
      " [0 4]]\n",
      "Training accuracy:  1.0\n",
      "[[1 0]\n",
      " [1 1]]\n",
      "Test accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "cm = my.confusion_matrix(train_1.examples.iloc[:,-1].values, dt_2.predict(train_1.examples.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(cm)\n",
    "print('Training accuracy: ', accuracy)\n",
    "\n",
    "cm = my.confusion_matrix(test_1.examples.iloc[:,-1].values, dt_2.predict(test_1.examples.iloc[:,:-1]))\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(cm)\n",
    "print('Test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess the results turned out the same as far as accuracy goes for the weighted entropy method as opposed to using the information gain. Of course, this is a small dataset so I'd be curious to see what the results would be for something larger. By the way, I just inherited from the first DecisionTree class and modified the necessary functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
